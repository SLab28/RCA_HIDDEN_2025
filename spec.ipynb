{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"},
  "language_info": {"name": "python", "version": "3.10.0"}
 },
 "cells": [
  {"cell_type": "markdown", "metadata": {}, "source": ["# Technical Specification: Interactive Point Cloud\n**RCA Exhibition — Mixed Reality Mobile Application**\n\n**Stack:** Three.js · WebGL GLSL · MediaPipe Hands · Web Audio API · GitHub Pages"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## 1. Overview\n\nA browser-based experience rendering an animated point cloud (~200k points) from Luma AI. The cloud responds in real time to audio, touch, and visitor hand presence via camera-based hand tracking. ~66k points participate in a flocking (boids) simulation. All per-point computation is GPU-bound via custom WebGL shaders."]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## 2. File Architecture\n\n```\nindex.html\n├── main.js                  # Scene orchestrator (Three.js)\n├── loaders/plyLoader.js     # PLY → BufferGeometry\n├── shaders/\n│   ├── pointCloud.vert      # Vertex shader\n│   └── pointCloud.frag      # Fragment shader\n├── systems/\n│   ├── audioSystem.js       # Web Audio API → uniforms\n│   ├── handTracker.js       # MediaPipe → hand position uniforms\n│   └── flockingSystem.js    # Boids → GPU texture\n└── assets/scan.ply\n```"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## 3. The Core Split: Three.js vs. Shader\n\n| Concern | Three.js (CPU) | Shader (GPU) |\n|---|---|---|\n| Load PLY, scene graph | ✓ | |\n| Update uniforms per frame | ✓ | |\n| Audio FFT analysis | ✓ | |\n| Hand position detection | ✓ | |\n| Boids neighbour lookup | ✓ (→ texture) | |\n| Per-point position update | | ✓ |\n| Per-point hand repulsion | | ✓ |\n| Per-point glow intensity | | ✓ |\n| Audio-driven displacement | | ✓ |\n| Point size & colour | | ✓ |"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## 4. Shader Uniforms Interface\n\nWhat Three.js passes to the GPU every frame:\n\n```glsl\nuniform float u_time;\nuniform float u_audioBass;      // 0.0–1.0\nuniform float u_audioMid;\nuniform float u_audioHigh;\nuniform vec3  u_handPos[2];     // World-space hand positions\nuniform float u_handPresence;   // 0.0–1.0\nuniform float u_handRadius;     // Influence radius\nuniform sampler2D u_flockTex;   // Encoded boids velocities\nuniform vec2  u_touchPos;\nuniform float u_touchActive;\nuniform float u_pointSize;\nuniform float u_glowIntensity;\n```"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## 5. Vertex Shader Logic (per-point, ~200k parallel)\n\n```\n1. Read base position from PLY geometry attribute\n2. If flocking point: sample u_flockTex, apply boids velocity\n3. Calculate distance to each hand\n4. If within u_handRadius: apply repulsion vector scaled by proximity\n5. Audio displacement:\n   Bass  → radial expansion\n   Mid   → noise shimmer\n   High  → vertical flutter\n6. Compute glow varying = 1 - (distToHand / u_handRadius)\n7. Output gl_Position + gl_PointSize (scaled by audio + depth)\n```"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## 6. Fragment Shader Logic\n\n```\n1. Discard corners → circular points\n2. Base colour from PLY vertex colour\n3. Glow: mix(baseColour, glowColour, vGlow * u_glowIntensity)\n4. Depth-based alpha fade\n```"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## 7. Flocking System\n\n- 66k points flagged via vertex attribute `a_isFlock`\n- Boids runs on CPU on a **proxy of max 5k agents** (performance)\n- Velocities encoded into a float texture → `u_flockTex`\n- Vertex shader reads texture, adds velocity to base position"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## 8. Hand Tracking System\n\n- Library: **MediaPipe Hands** (CDN, fully browser-side)\n- Detects up to 2 hands, 21 landmarks each\n- Key landmarks: wrist (0) + index fingertip (8)\n- Convert normalised MediaPipe coords → Three.js world space via camera unproject\n- Glow: `smoothstep(u_handRadius, 0.0, dist)` in fragment shader for soft falloff"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## 9. Performance Targets\n\n| Metric | Target |\n|---|---|\n| Frame rate | 60fps desktop / 30fps mobile |\n| Total points | 200,000 |\n| Flock points | ~66,000 |\n| Boids proxy agents | ≤ 5,000 |\n| Hand tracking latency | < 50ms |"]},
  {"cell_type": "markdown", "metadata": {}, "source": ["## 10. Open Questions\n\n1. Should flocking affect all 200k points or stay at 66k?\n2. Target device — desktop browser or mobile?\n3. Audio source — live microphone or pre-loaded track?\n4. Body presence detection needed, or hands only?\n5. ShaderPark — prototyping only, then port to raw GLSL for production?"]}
 ]
}